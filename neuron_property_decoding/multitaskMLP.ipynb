{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7539f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Loss: 60157.2138\n",
      "Epoch 2/100 | Loss: 59881.9865\n",
      "Epoch 3/100 | Loss: 59798.8733\n",
      "Epoch 4/100 | Loss: 59694.1574\n",
      "Epoch 5/100 | Loss: 59590.5330\n",
      "Epoch 6/100 | Loss: 59453.4997\n",
      "Epoch 7/100 | Loss: 59364.0451\n",
      "Epoch 8/100 | Loss: 59393.5859\n",
      "Epoch 9/100 | Loss: 59175.0939\n",
      "Epoch 10/100 | Loss: 59024.8571\n",
      "Epoch 11/100 | Loss: 58934.6073\n",
      "Epoch 12/100 | Loss: 58722.4626\n",
      "Epoch 13/100 | Loss: 58490.3666\n",
      "Epoch 14/100 | Loss: 58175.9805\n",
      "Epoch 15/100 | Loss: 57643.2842\n",
      "Epoch 16/100 | Loss: 57100.8867\n",
      "Epoch 17/100 | Loss: 56431.3131\n",
      "Epoch 18/100 | Loss: 55656.4530\n",
      "Epoch 19/100 | Loss: 54634.9316\n",
      "Epoch 20/100 | Loss: 53655.8958\n",
      "Epoch 21/100 | Loss: 52521.8843\n",
      "Epoch 22/100 | Loss: 51113.2843\n",
      "Epoch 23/100 | Loss: 49612.7865\n",
      "Epoch 24/100 | Loss: 55705.6016\n",
      "Epoch 25/100 | Loss: 46553.8355\n",
      "Epoch 26/100 | Loss: 44019.6302\n",
      "Epoch 27/100 | Loss: 42580.0331\n",
      "Epoch 28/100 | Loss: 40219.3167\n",
      "Epoch 29/100 | Loss: 37481.2137\n",
      "Epoch 30/100 | Loss: 36145.3332\n",
      "Epoch 31/100 | Loss: 34203.4747\n",
      "Epoch 32/100 | Loss: 31225.6236\n",
      "Epoch 33/100 | Loss: 29822.7476\n",
      "Epoch 34/100 | Loss: 27311.1780\n",
      "Epoch 35/100 | Loss: 25522.0386\n",
      "Epoch 36/100 | Loss: 23033.2481\n",
      "Epoch 37/100 | Loss: 20634.1330\n",
      "Epoch 38/100 | Loss: 18974.5718\n",
      "Epoch 39/100 | Loss: 16634.6673\n",
      "Epoch 40/100 | Loss: 15675.1950\n",
      "Epoch 41/100 | Loss: 13258.7219\n",
      "Epoch 42/100 | Loss: 12084.1514\n",
      "Epoch 43/100 | Loss: 10623.9008\n",
      "Epoch 44/100 | Loss: 9233.3189\n",
      "Epoch 45/100 | Loss: 8462.3115\n",
      "Epoch 46/100 | Loss: 7534.9520\n",
      "Epoch 47/100 | Loss: 6977.2985\n",
      "Epoch 48/100 | Loss: 6239.9724\n",
      "Epoch 49/100 | Loss: 5802.8729\n",
      "Epoch 50/100 | Loss: 5508.1981\n",
      "Epoch 51/100 | Loss: 5274.7577\n",
      "Epoch 52/100 | Loss: 5074.3615\n",
      "Epoch 53/100 | Loss: 4965.7316\n",
      "Epoch 54/100 | Loss: 4861.9625\n",
      "Epoch 55/100 | Loss: 4867.8180\n",
      "Epoch 56/100 | Loss: 4774.6037\n",
      "Epoch 57/100 | Loss: 4703.5384\n",
      "Epoch 58/100 | Loss: 4701.3603\n",
      "Epoch 59/100 | Loss: 4645.9912\n",
      "Epoch 60/100 | Loss: 4632.3974\n",
      "Epoch 61/100 | Loss: 4618.5258\n",
      "Epoch 62/100 | Loss: 4566.4630\n",
      "Epoch 63/100 | Loss: 4539.5444\n",
      "Epoch 64/100 | Loss: 4561.7447\n",
      "Epoch 65/100 | Loss: 4521.6045\n",
      "Epoch 66/100 | Loss: 4670.5253\n",
      "Epoch 67/100 | Loss: 4483.4616\n",
      "Epoch 68/100 | Loss: 4511.6699\n",
      "Epoch 69/100 | Loss: 4477.4923\n",
      "Epoch 70/100 | Loss: 4462.4675\n",
      "Epoch 71/100 | Loss: 4494.0144\n",
      "Epoch 72/100 | Loss: 4485.4823\n",
      "Epoch 73/100 | Loss: 4485.8649\n",
      "Epoch 74/100 | Loss: 4617.4155\n",
      "Epoch 75/100 | Loss: 4617.3509\n",
      "Epoch 76/100 | Loss: 4848.6222\n",
      "Epoch 77/100 | Loss: 4610.2683\n",
      "Epoch 78/100 | Loss: 4518.0981\n",
      "Epoch 79/100 | Loss: 4524.4709\n",
      "Epoch 80/100 | Loss: 4542.9062\n",
      "Epoch 81/100 | Loss: 4496.9878\n",
      "Epoch 82/100 | Loss: 4563.5016\n",
      "Epoch 83/100 | Loss: 4553.5435\n",
      "Epoch 84/100 | Loss: 4804.8836\n",
      "Epoch 85/100 | Loss: 4834.3254\n",
      "Epoch 86/100 | Loss: 5078.0781\n",
      "Epoch 87/100 | Loss: 4746.7769\n",
      "Epoch 88/100 | Loss: 4621.5631\n",
      "Epoch 89/100 | Loss: 4360.0414\n",
      "Epoch 90/100 | Loss: 4361.6426\n",
      "Epoch 91/100 | Loss: 4364.0140\n",
      "Epoch 92/100 | Loss: 4352.5209\n",
      "Epoch 93/100 | Loss: 4340.6191\n",
      "Epoch 94/100 | Loss: 4368.0242\n",
      "Epoch 95/100 | Loss: 4363.8410\n",
      "Epoch 96/100 | Loss: 4667.1486\n",
      "Epoch 97/100 | Loss: 4626.4787\n",
      "Epoch 98/100 | Loss: 4500.4939\n",
      "Epoch 99/100 | Loss: 4281.4687\n",
      "Epoch 100/100 | Loss: 4192.6647\n",
      "\n",
      "ðŸ“ˆ Variance explained per property:\n",
      "osi_dg: RÂ² = -0.0456\n",
      "dsi_dg: RÂ² = -0.0133\n",
      "g_osi_dg: RÂ² = -0.0368\n",
      "g_dsi_dg: RÂ² = -0.0236\n",
      "reliability_dg: RÂ² = -0.0822\n",
      "reliability_nm1_a: RÂ² = -0.1013\n",
      "run_mod_dg: RÂ² = -0.0832\n",
      "tfdi_dg: RÂ² = -0.0901\n",
      "pref_tf_dg: RÂ² = -0.0251\n",
      "peak_dff_dg: RÂ² = -0.5174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cell_df=\"/home/maria/LuckyMouse2/neuron_property_decoding/data/cell_metrics_joined.csv\"\n",
    "neuron_embeddings=\"/home/maria/LuckyMouse2/neuron_property_decoding/data/neuron_embeddings.npy\"\n",
    "cell_df=pd.read_csv(cell_df)\n",
    "neuron_embeddings=np.load(neuron_embeddings)\n",
    "merged_df=cell_df\n",
    "X=neuron_embeddings\n",
    "# Load embeddings (e.g. from your model output)\n",
    "#X = np.load(\"/path/to/your_neuron_embeddings.npy\")  # shape (N, 64)\n",
    "merged_df = merged_df.sort_values(\"data_row_index\").reset_index(drop=True)\n",
    "\n",
    "# Select targets\n",
    "targets = [\n",
    "    'osi_dg', 'dsi_dg', 'g_osi_dg', 'g_dsi_dg',\n",
    "    'reliability_dg', 'reliability_nm1_a',\n",
    "    'run_mod_dg', 'tfdi_dg', 'pref_tf_dg',\n",
    "    'peak_dff_dg'\n",
    "]\n",
    "Y_df = merged_df[targets]\n",
    "\n",
    "Y_df_filled = Y_df.fillna(0.0)\n",
    "Y = Y_df_filled.to_numpy(dtype=np.float32)\n",
    "class NeuronPropertyDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y, dtype=torch.float32)\n",
    "        self.mask = ~torch.isnan(self.Y)  # 1 where target exists\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx], self.mask[idx]\n",
    "    \n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Split data\n",
    "train_idx, test_idx = train_test_split(\n",
    "    np.arange(X.shape[0]), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = NeuronPropertyDataset(X[train_idx], Y[train_idx])\n",
    "test_dataset = NeuronPropertyDataset(X[test_idx], Y[test_idx])\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiTaskRegressor(input_dim=X.shape[1], output_dim=Y.shape[1]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def masked_mse_loss(pred, target, mask):\n",
    "    diff = pred - target\n",
    "    loss = (diff**2 * mask).sum() / mask.sum()\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch, mask_batch in train_loader:\n",
    "        x_batch, y_batch, mask_batch = x_batch.to(device), y_batch.to(device), mask_batch.to(device)\n",
    "        preds = model(x_batch)\n",
    "        loss = masked_mse_loss(preds, y_batch, mask_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test = torch.tensor(X[test_idx], dtype=torch.float32).to(device)\n",
    "    Y_test = torch.tensor(Y[test_idx], dtype=torch.float32).to(device)\n",
    "    mask_test = ~torch.isnan(Y_test)\n",
    "\n",
    "    preds = model(X_test)\n",
    "    preds = preds.cpu().numpy()\n",
    "    Y_test = Y_test.cpu().numpy()\n",
    "    mask_test = mask_test.cpu().numpy()\n",
    "\n",
    "# RÂ² for each task\n",
    "print(\"\\nðŸ“ˆ Variance explained per property:\")\n",
    "for i, name in enumerate(targets):\n",
    "    mask = mask_test[:, i]\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"{name}: No data\")\n",
    "        continue\n",
    "    r2 = r2_score(Y_test[mask, i], preds[mask, i])\n",
    "    print(f\"{name}: RÂ² = {r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1fea241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Variance explained per property:\n",
      "osi_dg: RÂ² = -0.5606\n",
      "dsi_dg: RÂ² = -0.0886\n",
      "g_osi_dg: RÂ² = -0.3425\n",
      "g_dsi_dg: RÂ² = -0.2840\n",
      "reliability_dg: RÂ² = -3.7606\n",
      "reliability_nm1_a: RÂ² = -1.6857\n",
      "run_mod_dg: RÂ² = -0.1101\n",
      "tfdi_dg: RÂ² = -0.4426\n",
      "pref_tf_dg: RÂ² = -0.0158\n",
      "peak_dff_dg: RÂ² = -0.8350\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nðŸ“ˆ Variance explained per property:\")\n",
    "for i, name in enumerate(targets):\n",
    "    mask = mask_test[:, i]\n",
    "    if mask.sum() == 0:\n",
    "        print(f\"{name}: No data\")\n",
    "        continue\n",
    "    r2 = r2_score(Y_test[mask, i], preds[mask, i])\n",
    "    print(f\"{name}: RÂ² = {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Global 3.10)",
   "language": "python",
   "name": "global-310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
